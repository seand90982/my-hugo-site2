<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>My New Hugo Site</title><link>http://localhost:1313/</link><description>Recent content on My New Hugo Site</description><generator>Hugo</generator><language>en-us</language><copyright>&amp;copy; 2025, Sean</copyright><lastBuildDate>Wed, 07 Jun 2023 11:59:59 -0400</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml"/><item><title>Physical Limits of AI Ex-Risk</title><link>http://localhost:1313/posts/ai-x-risk/</link><pubDate>Wed, 07 Jun 2023 11:59:59 -0400</pubDate><guid>http://localhost:1313/posts/ai-x-risk/</guid><description>&lt;h2 id="physical-limits-of-ai-existential-risk">Physical Limits of AI Existential Risk&lt;/h2>
&lt;p>&lt;a href="https://a16z.com/2023/06/06/ai-will-save-the-world/">Marc Andreesen dismisses&lt;/a> the threat of AI existential risk based on three reasons: self-puffery, paid punditry, and cults. I&amp;rsquo;m sure those are all factors, but that doesn&amp;rsquo;t engage at all with the actual mechanics of why. But I think his main argument is that the doomer predictions seem largely or entirely to be unscientific, untestable.
True enough, but I&amp;rsquo;d much prefer to try to make a positive argument for why AI is safe, rather than finding problems with the negative arguments.&lt;/p></description></item><item><title>You're Lucky if You've Seen Some Things Done Right, and Some Things Done Badly</title><link>http://localhost:1313/posts/youre-lucky/</link><pubDate>Mon, 08 Jul 2019 11:59:59 -0400</pubDate><guid>http://localhost:1313/posts/youre-lucky/</guid><description>&lt;h2 id="youre-lucky-if-youve-seen-some-things-done-right-and-some-things-done-badly">You&amp;rsquo;re Lucky if You&amp;rsquo;ve Seen Some Things Done Right, and Some Things Done Badly&lt;/h2>
&lt;p>A naive belief is that you&amp;rsquo;d be lucky to have only seen things done the right way, as if that&amp;rsquo;s some kind of assurance that you&amp;rsquo;ve only learned correct lessons.
If you&amp;rsquo;ve only ever seen things done the right way it doesn&amp;rsquo;t mean you&amp;rsquo;ve learned the right way. It means you&amp;rsquo;ve severely under-sampled the parameter space of success/method, and you probably underestimate the role of chance. You don&amp;rsquo;t know how far you can get from the right way before things go awry, and worse you may not know the signs of things going awry. Or even worse you may not appreciate the value of doing things the right way - i.e. you may not understand the compounding effects of bad (or even just subpar) practices.&lt;/p></description></item></channel></rss>