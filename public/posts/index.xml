<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on My New Hugo Site</title><link>http://localhost:1313/posts/</link><description>Recent content in Posts on My New Hugo Site</description><generator>Hugo</generator><language>en-us</language><copyright>&amp;copy; 2025 Sean</copyright><lastBuildDate>Wed, 07 Jun 2023 11:59:59 -0400</lastBuildDate><atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Physical Limits of AI Ex-Risk</title><link>http://localhost:1313/posts/ai-x-risk/</link><pubDate>Wed, 07 Jun 2023 11:59:59 -0400</pubDate><guid>http://localhost:1313/posts/ai-x-risk/</guid><description>&lt;h2 id="physical-limits-of-ai-existential-risk">Physical Limits of AI Existential Risk&lt;/h2>
&lt;p>&lt;a href="https://a16z.com/2023/06/06/ai-will-save-the-world/">Marc Andreesen dismisses&lt;/a> the threat of AI existential risk based on three reasons: self-puffery, paid punditry, and cults. I&amp;rsquo;m sure those are all factors, but that doesn&amp;rsquo;t engage at all with the actual mechanics of why. But I think his main argument is that the doomer predictions seem largely or entirely to be unscientific, untestable.
True enough, but I&amp;rsquo;d much prefer to try to make a positive argument for why AI is safe, rather than finding problems with the negative arguments.&lt;/p></description></item><item><title>Experimentation and Understanding</title><link>http://localhost:1313/posts/experimentation-and-understanding/</link><pubDate>Wed, 29 Jan 2020 11:59:59 -0400</pubDate><guid>http://localhost:1313/posts/experimentation-and-understanding/</guid><description>&lt;h2 id="experimentation-and-understanding">Experimentation and Understanding&lt;/h2>
&lt;p>&lt;a href="https://marginalrevolution.com/marginalrevolution/2020/01/draining-the-swamp.html">Tyler Cowen&amp;rsquo;s blurb&lt;/a> on Jason Crawford&amp;rsquo;s post &lt;a href="https://rootsofprogress.org/draining-the-swamp">Draining the swamp&lt;/a> points out that even before vaccination and antibiotics, tuberculosis, influenza/pneumonia, and gastroenteric diseases were on the decline. This was due to making efforts on sanitation, which was &amp;ldquo;based on data collection and analysis, long before a full scientific theory of infection had been worked out.&amp;rdquo;&lt;/p>
&lt;p>I think this is a very important point. Many people, if only implicitly, take the view that most progress comes from breakthroughs. That is, that we may toil for a while, making tiny amounts of progress, which maybe we can&amp;rsquo;t even hold on to because we just donâ€™t know what we&amp;rsquo;re doing and have no real understanding of the underlying problem, etc, etc, and then one day we have a breakthrough, a major advance in understanding and knowledge, and finally we&amp;rsquo;re able to really put that problem behind us - from that point it&amp;rsquo;s just a matter of implementation and convincing the skeptics, and of doing away with the old and backwards ways of thinking.&lt;/p></description></item><item><title>You're Lucky if You've Seen Some Things Done Right, and Some Things Done Badly</title><link>http://localhost:1313/posts/youre-lucky/</link><pubDate>Mon, 08 Jul 2019 11:59:59 -0400</pubDate><guid>http://localhost:1313/posts/youre-lucky/</guid><description>&lt;h2 id="youre-lucky-if-youve-seen-some-things-done-right-and-some-things-done-badly">You&amp;rsquo;re Lucky if You&amp;rsquo;ve Seen Some Things Done Right, and Some Things Done Badly&lt;/h2>
&lt;p>A naive belief is that you&amp;rsquo;d be lucky to have only seen things done the right way, as if that&amp;rsquo;s some kind of assurance that you&amp;rsquo;ve only learned correct lessons.
If you&amp;rsquo;ve only ever seen things done the right way it doesn&amp;rsquo;t mean you&amp;rsquo;ve learned the right way. It means you&amp;rsquo;ve severely under-sampled the parameter space of success/method, and you probably underestimate the role of chance. You don&amp;rsquo;t know how far you can get from the right way before things go awry, and worse you may not know the signs of things going awry. Or even worse you may not appreciate the value of doing things the right way - i.e. you may not understand the compounding effects of bad (or even just subpar) practices.&lt;/p></description></item></channel></rss>