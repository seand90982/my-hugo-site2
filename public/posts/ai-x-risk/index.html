<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><title>My New Hugo Site</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=description content><link rel=stylesheet href=../../css/theme.min.css></head><body><div id=content class=mx-auto><header class="container mt-sm-5 mt-4 mb-4 mt-xs-1"><div class=row><div class="col-12 text-center"><h2 class="m-0 mb-2 mt-4"><a href=../../ class=text-decoration-none>Sean</a></h2><p class="text-muted mb-1">-----</p><ul id=nav-links class="list-inline mb-2"><li class=list-inline-item><a class="badge badge-white" href=../../about/ title=About>About</a></li><li class=list-inline-item><a class="badge badge-white" href=../../post/ title=Posts>Posts</a></li><li class=list-inline-item><a class="badge badge-white" href=../../categories/ title=Categories>Categories</a></li></ul><ul id=nav-social class=list-inline></ul></div></div><hr></header><div class=container><div class="pl-sm-4 ml-sm-5"><h2 id=physical-limits-of-ai-existential-risk>Physical Limits of AI Existential Risk</h2><p><a href=https://a16z.com/2023/06/06/ai-will-save-the-world/>Marc Andreesen dismisses</a> the threat of AI existential risk based on three reasons: self-puffery, paid punditry, and cults. I&rsquo;m sure those are all factors, but that doesn&rsquo;t engage at all with the actual mechanics of why. But I think his main argument is that the doomer predictions seem largely or entirely to be unscientific, untestable.
True enough, but I&rsquo;d much prefer to try to make a positive argument for why AI is safe, rather than finding problems with the negative arguments.</p><p>Why do I think the AI doom scenario is exceptionally unlikely (maybe impossible)? It&rsquo;s because however powerful or intelligent any AI is, or how fast it can become still more intelligent, it&rsquo;s ability to impact the actual physical world is going to be limited by what it can physically do.</p><p>Take a stupid example: my stapler may or may not have a mind of its own, but assume that it does. It can shoot out sharp little bits of metal any time it wants. What stops it from doing so? It has no muscles, no actuators, no way of effecting on its own any physical movement whatsoever. Same with my computer - it can spin its fans, make some sound, get warm, or turn on its little power light. My old laptop could even eject a CD. They were limited by their design.</p><p>Likewise, we can and should understand that any AI-driven system could malfunction, and thus build in physical limits on what any system can do on its own. We do this all the time. Forklifts can be pretty dangerous, so they have speed limiters, tip sensors, load sensors, etc. which set limits on what they can do. They also have brake pedals, and emergency stop buttons. EMO buttons, as they&rsquo;re called (emergency-off), are designed into many systems and machines.</p><p>One response might be that an AI system could have much wider influence than a single machine - it could be placed in control of a very large piece of infrastructure, or may be connected to a network or even the wider Internet in order to communicate with and control all sorts of other things. Power grids or transportation systems, for example. But these systems are all still limited by the physical capabilities designed into them. We can and should build interlocks and monitoring systems to make sure these systems operate correctly, and cannot be taken over by unauthorized entities. But this is true with or without AI - there are plenty of nefarious human actors to guard against.</p></div></div></div><footer class="text-center pb-1"><small class=text-muted>&copy; 2025, Sean<br>Powered by <a href=https://gohugo.io/ target=_blank>Hugo</a>
and <a href=https://github.com/austingebauer/devise target=_blank>Devise</a></small></footer></body></html>